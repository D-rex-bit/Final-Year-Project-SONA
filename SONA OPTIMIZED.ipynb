{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b6a316-c33d-48eb-88cb-0b65aade90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5e9583-c55f-4e15-9af0-8865070f6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fa32409-7678-4267-890a-d1d884c50613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08447b01-5fde-4f57-84e2-dbdc771c9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_claims(claims):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    preprocessed_claims = []\n",
    "    for claim in claims:\n",
    "        # Remove non-alphanumeric characters\n",
    "        claim = re.sub(r'[^a-zA-Z0-9]', ' ', claim)\n",
    "        # Lowercase\n",
    "        claim = claim.lower()\n",
    "        # Tokenization and Lemmatization\n",
    "        claim = ' '.join(lemmatizer.lemmatize(word) for word in claim.split() if word not in STOPWORDS)\n",
    "        preprocessed_claims.append(claim)\n",
    "        \n",
    "    return preprocessed_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1281ea4-13ba-43d2-bde8-4f50cc13d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate models\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on training and testing sets\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Store the results\n",
    "        results[model_name] = {\n",
    "            'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'train_confusion_matrix': confusion_matrix(y_train, y_train_pred),\n",
    "            'test_confusion_matrix': confusion_matrix(y_test, y_test_pred),\n",
    "            'train_classification_report': classification_report(y_train, y_train_pred, output_dict=True),\n",
    "            'test_classification_report': classification_report(y_test, y_test_pred, output_dict=True)\n",
    "        }\n",
    "        # saving the models\n",
    "        joblib.dump(model, f'{model_name.lower().replace(\" \", \"_\")}_model.pkl')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba26fd6e-ce47-40bf-8dbc-3291ca64dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute the workflow\n",
    "def main(file_path):\n",
    "    # Load data\n",
    "    df = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76654388-1528-4bbb-adb6-215b1f33143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Training Accuracy: 0.9909\n",
      "Testing Accuracy: 0.9330\n",
      "Training Confusion Matrix:\n",
      "[[  95    0]\n",
      " [  19 1973]]\n",
      "Testing Confusion Matrix:\n",
      "[[  1  24]\n",
      " [ 11 486]]\n",
      "Training Classification Report:\n",
      "{'0': {'precision': 0.8333333333333334, 'recall': 1.0, 'f1-score': 0.9090909090909091, 'support': 95.0}, '1': {'precision': 1.0, 'recall': 0.9904618473895582, 'f1-score': 0.9952080706179067, 'support': 1992.0}, 'accuracy': 0.9908960229995208, 'macro avg': {'precision': 0.9166666666666667, 'recall': 0.9952309236947792, 'f1-score': 0.9521494898544078, 'support': 2087.0}, 'weighted avg': {'precision': 0.9924133524996006, 'recall': 0.9908960229995208, 'f1-score': 0.991288027328465, 'support': 2087.0}}\n",
      "Testing Classification Report:\n",
      "{'0': {'precision': 0.08333333333333333, 'recall': 0.04, 'f1-score': 0.05405405405405406, 'support': 25.0}, '1': {'precision': 0.9529411764705882, 'recall': 0.9778672032193159, 'f1-score': 0.9652432969215492, 'support': 497.0}, 'accuracy': 0.9329501915708812, 'macro avg': {'precision': 0.5181372549019607, 'recall': 0.508933601609658, 'f1-score': 0.5096486754878016, 'support': 522.0}, 'weighted avg': {'precision': 0.9112932912628652, 'recall': 0.9329501915708812, 'f1-score': 0.9216039653665925, 'support': 522.0}}\n",
      "--------------------------------------------------\n",
      "Model: Logistic Regression\n",
      "Training Accuracy: 0.9693\n",
      "Testing Accuracy: 0.9157\n",
      "Training Confusion Matrix:\n",
      "[[  95    0]\n",
      " [  64 1928]]\n",
      "Testing Confusion Matrix:\n",
      "[[  3  22]\n",
      " [ 22 475]]\n",
      "Training Classification Report:\n",
      "{'0': {'precision': 0.5974842767295597, 'recall': 1.0, 'f1-score': 0.7480314960629921, 'support': 95.0}, '1': {'precision': 1.0, 'recall': 0.9678714859437751, 'f1-score': 0.9836734693877551, 'support': 1992.0}, 'accuracy': 0.9693339722089123, 'macro avg': {'precision': 0.7987421383647799, 'recall': 0.9839357429718876, 'f1-score': 0.8658524827253736, 'support': 2087.0}, 'weighted avg': {'precision': 0.9816775305650733, 'recall': 0.9693339722089123, 'f1-score': 0.9729470738602743, 'support': 2087.0}}\n",
      "Testing Classification Report:\n",
      "{'0': {'precision': 0.12, 'recall': 0.12, 'f1-score': 0.12, 'support': 25.0}, '1': {'precision': 0.9557344064386318, 'recall': 0.9557344064386318, 'f1-score': 0.9557344064386318, 'support': 497.0}, 'accuracy': 0.9157088122605364, 'macro avg': {'precision': 0.5378672032193159, 'recall': 0.5378672032193159, 'f1-score': 0.5378672032193159, 'support': 522.0}, 'weighted avg': {'precision': 0.9157088122605364, 'recall': 0.9157088122605364, 'f1-score': 0.9157088122605364, 'support': 522.0}}\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Training Accuracy: 0.9909\n",
      "Testing Accuracy: 0.8410\n",
      "Training Confusion Matrix:\n",
      "[[  95    0]\n",
      " [  19 1973]]\n",
      "Testing Confusion Matrix:\n",
      "[[  6  19]\n",
      " [ 64 433]]\n",
      "Training Classification Report:\n",
      "{'0': {'precision': 0.8333333333333334, 'recall': 1.0, 'f1-score': 0.9090909090909091, 'support': 95.0}, '1': {'precision': 1.0, 'recall': 0.9904618473895582, 'f1-score': 0.9952080706179067, 'support': 1992.0}, 'accuracy': 0.9908960229995208, 'macro avg': {'precision': 0.9166666666666667, 'recall': 0.9952309236947792, 'f1-score': 0.9521494898544078, 'support': 2087.0}, 'weighted avg': {'precision': 0.9924133524996006, 'recall': 0.9908960229995208, 'f1-score': 0.991288027328465, 'support': 2087.0}}\n",
      "Testing Classification Report:\n",
      "{'0': {'precision': 0.08571428571428572, 'recall': 0.24, 'f1-score': 0.12631578947368421, 'support': 25.0}, '1': {'precision': 0.9579646017699115, 'recall': 0.8712273641851107, 'f1-score': 0.9125395152792413, 'support': 497.0}, 'accuracy': 0.8409961685823755, 'macro avg': {'precision': 0.5218394437420986, 'recall': 0.5556136820925553, 'f1-score': 0.5194276523764627, 'support': 522.0}, 'weighted avg': {'precision': 0.9161901613457915, 'recall': 0.8409961685823755, 'f1-score': 0.8748851222808908, 'support': 522.0}}\n",
      "--------------------------------------------------\n",
      "Model: Gradient Boosting\n",
      "Training Accuracy: 0.9612\n",
      "Testing Accuracy: 0.9444\n",
      "Training Confusion Matrix:\n",
      "[[  14   81]\n",
      " [   0 1992]]\n",
      "Testing Confusion Matrix:\n",
      "[[  0  25]\n",
      " [  4 493]]\n",
      "Training Classification Report:\n",
      "{'0': {'precision': 1.0, 'recall': 0.14736842105263157, 'f1-score': 0.25688073394495414, 'support': 95.0}, '1': {'precision': 0.9609261939218524, 'recall': 1.0, 'f1-score': 0.9800738007380074, 'support': 1992.0}, 'accuracy': 0.9611883085769046, 'macro avg': {'precision': 0.9804630969609263, 'recall': 0.5736842105263158, 'f1-score': 0.6184772673414808, 'support': 2087.0}, 'weighted avg': {'precision': 0.9627048290811356, 'recall': 0.9611883085769046, 'f1-score': 0.9471541355030577, 'support': 2087.0}}\n",
      "Testing Classification Report:\n",
      "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 25.0}, '1': {'precision': 0.9517374517374517, 'recall': 0.9919517102615694, 'f1-score': 0.9714285714285714, 'support': 497.0}, 'accuracy': 0.9444444444444444, 'macro avg': {'precision': 0.47586872586872586, 'recall': 0.4959758551307847, 'f1-score': 0.4857142857142857, 'support': 522.0}, 'weighted avg': {'precision': 0.9061561561561561, 'recall': 0.9444444444444444, 'f1-score': 0.924904214559387, 'support': 522.0}}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Main function to execute the workflow\n",
    "def main(file_path):\n",
    "    # Load data\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Preprocess claims\n",
    "    df['claims'] = preprocess_claims(df['claims'])\n",
    "    \n",
    "    # Encode speaker labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['encoded_speaker'] = label_encoder.fit_transform(df['speaker'])\n",
    "    \n",
    "    # Vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_claims = tfidf_vectorizer.fit_transform(df['claims']).toarray()\n",
    "    \n",
    "    # Combine TF-IDF matrix and encoded speaker matrix\n",
    "    X_speaker = df['encoded_speaker'].values.reshape(-1, 1)\n",
    "    X_combined = np.hstack((X_claims, X_speaker))\n",
    "    y = df['label']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # saving the vectorizer\n",
    "    joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Training Accuracy: {metrics['train_accuracy']:.4f}\")\n",
    "        print(f\"Testing Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(\"Training Confusion Matrix:\")\n",
    "        print(metrics['train_confusion_matrix'])\n",
    "        print(\"Testing Confusion Matrix:\")\n",
    "        print(metrics['test_confusion_matrix'])\n",
    "        print(\"Training Classification Report:\")\n",
    "        print(metrics['train_classification_report'])\n",
    "        print(\"Testing Classification Report:\")\n",
    "        print(metrics['test_classification_report'])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #path to your dataset\n",
    "    file_path = 'sonadataset.csv'\n",
    "    main(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
